# -*- coding: utf-8 -*-
"""Loop Unrolling and JIT Compilation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mo_V_B6UcYNtLlvWcXMeivSQoCZQRY9_
"""

"""
Day 2, Session 2, Topic 2: Loop Unrolling and JIT (via Numba, Numexpr)

This file provides a deep dive into loop unrolling and Just-In-Time (JIT)
compilation techniques for optimizing numerical computations in Python.
It focuses on using Numba and Numexpr to accelerate CPU-bound tasks.
"""

import numpy as np
import time
from numba import jit, njit, prange, set_num_threads  # Import Numba decorators and functions
import numexpr as ne  # Import Numexpr
import os

# -----------------------------------------------------------------------------
# Loop Unrolling
# -----------------------------------------------------------------------------
#
# Loop unrolling is a loop optimization technique that reduces the overhead
# of loop control instructions (e.g., incrementing the loop counter,
# checking the loop condition) by increasing the number of operations
# performed within each iteration.
#
# Why use loop unrolling?
#
# -   Reduced loop overhead: By performing multiple operations per iteration,
#     the proportion of time spent on loop control is decreased, leading to
#     faster execution.
# -   Improved instruction-level parallelism (ILP): Unrolling can expose
#     more independent instructions to the CPU, allowing it to execute
#     them in parallel (if the CPU supports it).
# -   Better cache utilization: Unrolling can improve data locality,
#     increasing the chances that data accessed in subsequent iterations
#     is already in the CPU cache.
#
# How loop unrolling works:
#
# The basic idea is to replicate the loop body multiple times within the loop,
# adjusting the loop counter accordingly.
#
# Example: Loop unrolling
def demonstrate_loop_unrolling(size=1000000):
    a = np.random.rand(size)
    b = np.random.rand(size)
    result_unrolled = np.zeros(size)
    result_normal = np.zeros(size)

    # Original loop
    def normal_loop(x, y, out):
        for i in range(len(x)):
            out[i] = x[i] + y[i]

    # Unrolled loop (unrolling factor of 4)
    def unrolled_loop(x, y, out):
        for i in range(0, len(x) - 3, 4):
            out[i] = x[i] + y[i]
            out[i + 1] = x[i + 1] + y[i + 1]
            out[i + 2] = x[i + 2] + y[i + 2]
            out[i + 3] = x[i + 3] + y[i + 3]
        for i in range(len(x) - (len(x) % 4), len(x)):  # Handle remaining elements
            out[i] = x[i] + y[i]

    # Time the normal loop
    start_time = time.time()
    normal_loop(a, b, result_normal)
    normal_time = time.time() - start_time
    print(f"Normal loop time: {normal_time:.4f} seconds")

    # Time the unrolled loop
    start_time = time.time()
    unrolled_loop(a, b, result_unrolled)
    unrolled_time = time.time() - start_time
    print(f"Unrolled loop time: {unrolled_time:.4f} seconds")
    print(f"Speedup: {normal_time / unrolled_time:.2f}x")
    np.testing.assert_allclose(result_normal, result_unrolled)
    return a,b

# -----------------------------------------------------------------------------
# JIT Compilation with Numba
# -----------------------------------------------------------------------------
#
# Just-In-Time (JIT) compilation is a technique that compiles code during
# runtime, rather than ahead of time (AOT).  Numba is a library that uses
# JIT compilation to accelerate Python functions, especially those that
# perform numerical computations.
#
# How Numba works:
#
# 1.  Decorator: You use the `@jit` decorator (or one of its variants) to
#     mark a Python function for JIT compilation.
#
# 2.  Type inference: When the decorated function is called, Numba analyzes
#     the input arguments to infer their data types.
#
# 3.  Compilation: Numba translates the Python code of the function into
#     optimized machine code (using the LLVM compiler infrastructure).
#     This machine code is specific to your CPU architecture.
#
# 4.  Caching: Numba caches the compiled machine code, so subsequent calls
#     to the function with the same input types are much faster (no
#     recompilation is needed).
#
# Advantages of Numba:
#
# -   Performance: Numba can significantly speed up numerical code, often
#     achieving performance comparable to C or Fortran.
# -   Ease of use:  You can often accelerate your code simply by adding a
#     decorator, without having to rewrite it in another language.
# -   NumPy integration:  Numba works seamlessly with NumPy arrays and
#     functions.
# -   Parallelization: Numba provides features for automatic parallelization.
#
# Numba modes:
#
# -   nopython mode:  This is the recommended mode.  When Numba can
#     compile the entire function to native code, it runs very fast.
#     Numba replaces the entire function with the compiled version.
# -   object mode:  If Numba cannot compile the entire function to
#     native code (e.g., if it encounters unsupported Python features),
#     it falls back to object mode.  In this mode, Numba operates on
#     Python objects, which is slower than nopython mode but can still
#     provide some speedup.  Numba acts more like a wrapper around the
#     original Python code in this mode.
#
# Example: Using Numba's @jit decorator
def demonstrate_numba(x, y):

    # Python function (slow)
    def python_loop(x, y):
        result = np.zeros_like(x)
        for i in range(len(x)):
            result[i] = x[i] + y[i]
        return result

    # Numba-jitted function (nopython mode)
    @jit(nopython=True)  # Or @njit, which is a shortcut for @jit(nopython=True)
    def numba_loop_nopython(x, y):
        result = np.zeros_like(x)
        for i in range(len(x)):
            result[i] = x[i] + y[i]
        return result

    # Numba-jitted function (object mode) - Example where nopython fails
    @jit  # Numba will automatically use object mode if nopython fails
    def numba_loop_object(x, y):
        result = np.zeros_like(x)
        for i in range(len(x)):
            result[i] = x[i] + y[i]
        return result

    # Time the Python loop
    start_time = time.time()
    result_python = python_loop(x, y)
    python_time = time.time() - start_time
    print(f"Python loop time: {python_time:.4f} seconds")

    # Time the Numba nopython loop
    start_time = time.time()
    result_numba_nopython = numba_loop_nopython(x, y)
    numba_nopython_time = time.time() - start_time
    print(f"Numba nopython time: {numba_nopython_time:.4f} seconds")
    print(f"Numba nopython speedup: {python_time / numba_nopython_time:.2f}x")

    # Time the Numba object mode loop
    start_time = time.time()
    result_numba_object = numba_loop_object(x, y)
    numba_object_time = time.time() - start_time
    print(f"Numba object time: {numba_object_time:.4f} seconds")
    print(f"Numba object speedup: {python_time / numba_object_time:.2f}x")

    np.testing.assert_allclose(result_python, result_numba_nopython)
    np.testing.assert_allclose(result_python, result_numba_object)
    return python_time, numba_nopython_time, numba_object_time
# -----------------------------------------------------------------------------
# JIT Compilation with Numba - Parallelization
# -----------------------------------------------------------------------------
#
# Numba also provides features for automatic parallelization, which can
# further improve performance on multi-core CPUs.
#
# Example: Numba with parallelization
def demonstrate_numba_parallel(x, y):
    # Use the number of available CPU cores, but cap it at 2 (as per the error message)
    num_threads = min(os.cpu_count(), 2)
    set_num_threads(num_threads)
    print(f"Using {num_threads} threads for Numba parallel execution.")

    @jit(nopython=True, parallel=True)
    def numba_parallel_loop(x, y):
        result = np.zeros_like(x)
        for i in prange(len(x)):  # Use prange instead of range for parallelization
            result[i] = x[i] + y[i]
        return result

    start_time = time.time()
    result_numba_parallel = numba_parallel_loop(x, y)
    numba_parallel_time = time.time() - start_time
    print(f"Numba parallel time: {numba_parallel_time:.4f} seconds")

    return numba_parallel_time
# -----------------------------------------------------------------------------
# Numexpr
# -----------------------------------------------------------------------------
#
# Numexpr is a library that accelerates the evaluation of numerical
# expressions involving NumPy arrays.  It uses a virtual machine to
# parse and execute expressions, often achieving significant speedups
# compared to NumPy's built-in evaluation.
#
# How Numexpr works:
#
# 1.  Expression parsing:  You provide Numexpr with a string representing
#     the numerical expression (e.g., "a + b * c").  Numexpr parses
#     this expression and creates an internal representation of the
#     computation.
#
# 2.  Virtual machine:  Numexpr uses a small virtual machine (VM) to
#     execute the operations in the expression.  The VM is optimized
#     for efficient execution on NumPy arrays.
#
# 3.  Memory management:  Numexpr can perform computations in-place or
#     allocate temporary arrays as needed.  It tries to minimize
#     memory allocation to improve performance.
#
# 4.  Vectorization and threading: Numexpr uses vectorization and can
#     also use multiple CPU cores to speed up computations.
#
# When to use Numexpr:
#
# -   Complex expressions: Numexpr is most beneficial for evaluating
#     complex numerical expressions involving multiple operations.
# -   Large arrays:  Numexpr performs best when working with large NumPy
#     arrays.
# -   Memory-bound operations:  Numexpr can be particularly effective
#     for memory-bound operations, where the speed of memory access
#     is the bottleneck.
#
# Example: Using Numexpr
def demonstrate_numexpr(x, y, z):
    # NumPy evaluation
    start_time = time.time()
    result_numpy = x + y * z
    numpy_time = time.time() - start_time
    print(f"NumPy time: {numpy_time:.4f} seconds")

    # Numexpr evaluation
    start_time = time.time()
    result_numexpr = ne.evaluate("x + y * z")  # The expression is a string
    numexpr_time = time.time() - start_time
    print(f"Numexpr time: {numexpr_time:.4f} seconds")
    print(f"Numexpr speedup: {numpy_time / numexpr_time:.2f}x")
    np.testing.assert_allclose(result_numpy, result_numexpr)
    return numpy_time, numexpr_time

# -----------------------------------------------------------------------------
# When to Use/Not Use Loop Unrolling and JIT Compilation
# -----------------------------------------------------------------------------
#
# When to Use Loop Unrolling:
#
# -   CPU-bound loops: Loop unrolling is most effective for CPU-bound loops
#     where the loop body contains a small amount of work and the loop
#     overhead is significant.
# -   Simple operations: Unrolling is best suited for loops with simple
#     arithmetic or memory access patterns.
# -   Known trip count: Unrolling is easier to apply when the number of
#     iterations is known or can be easily determined.
#
# When Not to Use Loop Unrolling:
#
# -   Complex loop bodies: If the loop body is very complex (e.g., contains
#     many function calls or conditional statements), the benefits of
#     unrolling might be reduced, and the increased code size could hurt.
# -   Small loop trip counts: For loops with a small number of iterations,
#     the overhead of the unrolled loop setup might outweigh the
#     performance gains.
# -   Compiler optimization: Modern compilers often perform loop unrolling
#     automatically, so manual unrolling might not be necessary.
#
# When to Use JIT Compilation (Numba):
#
# -   Numerical code: Numba is excellent for accelerating numerical code
#     that operates on NumPy arrays.
# -   CPU-bound functions: Numba works best for CPU-bound functions where
#     the majority of the time is spent in computation rather than I/O
#     or other operations.
# -   Python loops: Numba can significantly speed up Python loops,
#     especially those with numerical operations.
# -   Existing NumPy code: Numba integrates well with NumPy, allowing you
#     to accelerate your existing NumPy-based code with minimal changes.
#
# When Not to Use JIT Compilation (Numba):
#
# -   I/O-bound code: Numba does not accelerate I/O-bound code (e.g.,
#     reading from or writing to files, network operations).
# -   Code with many Python object manipulations: Numba's `nopython`
#     mode, which provides the best performance, does not support all
#     Python features.  If your code relies heavily on Python object
#     manipulations, Numba might fall back to `object` mode, which is
#     slower.
# -   Small functions: For very small functions, the overhead of JIT
#     compilation might outweigh the performance benefits.  However,
#     Numba's caching mechanism mitigates this issue for repeated calls.
#
# When to Use Numexpr:
#
# -   Complex numerical expressions: Numexpr is ideal for accelerating
#     the evaluation of complex arithmetic expressions involving NumPy
#     arrays.
# -   Memory-bound operations: Numexpr can be very effective for
#     memory-bound operations where the speed of memory access is the
#     bottleneck.
# -   Large arrays: Numexpr performs best when working with large NumPy
#     arrays.
#
# When Not to Use Numexpr:
#
# -   Simple expressions: For very simple expressions (e.g., a single
#     addition or multiplication), the overhead of Numexpr might
#     outweigh the benefits.
# -   Non-numerical operations: Numexpr is designed for numerical
#     computations.  It does not support other types of operations,
#     such as string manipulation or I/O.
# -   Code that is already heavily optimized: If your code is already
#     highly optimized (e.g., using carefully crafted NumPy
#     vectorization), Numexpr might not provide significant additional
#     speedup.
#
# Alternatives to Consider:
#
# -   Cython: A language that allows you to write C code that can
#     interact with Python.  Cython can provide significant performance
#     improvements, but it requires writing code in a separate language.
# -   Pythran: A Python-to-C++ compiler that can accelerate numerical
#     code.  Pythran can be easier to use than Cython for some types
#     of code.
# -   Libraries like PyTorch or TensorFlow: For GPU-accelerated
#     computing, consider using libraries like PyTorch or TensorFlow.
#
# -----------------------------------------------------------------------------
# Challenge: Optimize a numerical computation with Numba or Numexpr
# -----------------------------------------------------------------------------
#
# Challenge Description:
#
# You are given two large NumPy arrays, `x` and `y`, of the same shape.  You need to
# perform the following computation:
#
#   result[i, j] = exp(-(x[i, j] - y[i, j])**2) / (1 + (x[i, j] + y[i, j])**2)
#
# Your task is to write a function that performs this computation as efficiently
# as possible.  Optimize the function using either Numba or Numexpr (or both).
#
# Input:
#
# -   `x`: A 2D NumPy array of shape (m, n) and data type `np.float64`.
# -   `y`: A 2D NumPy array of shape (m, n) and data type `np.float64`.
#
# Output:
#
# -   A 2D NumPy array of shape (m, n) and data type `np.float64`, containing the
#     result of the computation.
#
# Example:
#
# x = np.array([[1.0, 2.0], [3.0, 4.0]])
# y = np.array([[0.5, 1.5], [2.5, 3.5]])
#
# result = your_optimized_function(x, y)
#
# Solution:
#
# Using Numexpr:
def optimized_computation_numexpr(x, y):
    """
    Performs the computation using Numexpr.

    Args:
        x: A 2D NumPy array (np.float64).
        y: A 2D NumPy array (np.float64).

    Returns:
        A 2D NumPy array (np.float64) containing the result.
    """
    return ne.evaluate("exp(-(x - y)**2) / (1 + (x + y)**2)")

# Using Numba
@jit(nopython=True)
def optimized_computation_numba(x, y):
    """
    Performs the computation using Numba.

    Args:
        x: A 2D NumPy array (np.float64).
        y: A 2D NumPy array (np.float64).

    Returns:
        A 2D NumPy array (np.float64) containing the result.
    """
    result = np.empty_like(x)
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            result[i, j] = np.exp(-(x[i, j] - y[i, j])**2) / (1 + (x[i, j] + y[i, j])**2)
    return result



if __name__ == "__main__":
    print("Exploring Loop Unrolling:")
    a,b = demonstrate_loop_unrolling()
    print("\nExploring Numba:")
    python_time, numba_nopython_time, numba_object_time = demonstrate_numba(a, b)
    numba_parallel_time = demonstrate_numba_parallel(a,b)
    print(f"Numba parallel speedup: {python_time / numba_parallel_time:.2f}x")
    x = np.random.rand(1000, 1000)  # Large arrays for Numexpr demo
    y = np.random.rand(1000, 1000)
    z = np.random.rand(1000,1000)
    print("\nExploring Numexpr:")
    numpy_time, numexpr_time = demonstrate_numexpr(x, y, z)

    print("\nRunning Challenge:")
    # Create sample data
    x_challenge = np.random.rand(1000, 1000)
    y_challenge = np.random.rand(1000, 1000)

    # Time the Numexpr solution
    start_time = time.time()
    result_numexpr_challenge = optimized_computation_numexpr(x_challenge, y_challenge)
    numexpr_time_challenge = time.time() - start_time
    print(f"Numexpr solution time: {numexpr_time_challenge:.4f} seconds")

    # Time the Numba solution
    start_time = time.time()
    result_numba_challenge = optimized_computation_numba(x_challenge, y_challenge)
    numba_time_challenge = time.time() - start_time
    print(f"Numba solution time: {numba_time_challenge:.4f} seconds")
    print(f"Numexpr vs Numba Speedup: {numba_time_challenge/numexpr_time_challenge:.2f}x")

    # Verify the results are the same (within a tolerance)
    np.testing.assert_allclose(result_numexpr_challenge, result_numba_challenge, rtol=1e-5)