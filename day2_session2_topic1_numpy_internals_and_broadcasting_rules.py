# -*- coding: utf-8 -*-
"""NumPy Internals and Broadcasting Rules

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXueM33umfdoT9cyvvmpDTjp1ZcSde8G
"""

"""
Day 2, Session 2, Topic 1: NumPy Internals and Broadcasting Rules

This file provides a deep dive into NumPy internals and broadcasting rules,
explaining the underlying mechanisms and best practices for efficient
numerical computation in Python.  It is designed for those who want
to become experts in Python numerical computing.
"""

import numpy as np
import sys
import ctypes  # Used for examining memory addresses

# -----------------------------------------------------------------------------
# NumPy Internals: The ndarray Object
# -----------------------------------------------------------------------------
#
# The core of NumPy is the `ndarray` object, which represents an n-dimensional array
# of homogeneous data. Understanding its internal structure is crucial for
# optimizing performance and avoiding common pitfalls.
#
# Key Attributes of an ndarray:
#
# 1.  data: A pointer to the memory buffer that stores the array's elements.
#     This is where the actual numerical data is located.
#
# 2.  dtype: Describes the type of elements in the array (e.g., int32, float64).
#     All elements in an ndarray have the same data type.  This homogeneity
#     is a key reason for NumPy's efficiency.
#
# 3.  shape: A tuple of integers indicating the size of the array in each
#     dimension. For example, a 2x3 matrix has a shape of (2, 3).
#
# 4.  strides: A tuple of integers indicating the number of bytes to step
#     in each dimension when traversing the array. Strides are crucial for
#     understanding how NumPy accesses elements in memory, especially for
#     non-contiguous arrays.
#
# 5.  flags: Information about the memory layout of the array (e.g.,
#     C_CONTIGUOUS, F_CONTIGUOUS, WRITEABLE).  Contiguity affects performance.
#
# Memory Layout (C-contiguous vs. F-contiguous):
#
# NumPy arrays can be stored in memory in two primary ways:
#
# -   C-contiguous:  Elements in the same row are stored next to each other
#     in memory.  This is the default layout in Python.  The last dimension
#     varies fastest.
# -   F-contiguous (Fortran contiguous): Elements in the same column are
#     stored next to each other in memory. The first dimension varies
#     fastest.
#
# The choice of memory layout can significantly impact performance,
# especially for operations that access elements in a specific order.
#
# Visual Representation (Conceptual):
#
# For a 2D array (3x4):
#
# C-contiguous:
#
# Memory: [0, 1, 2, 3,  4, 5, 6, 7,  8, 9, 10, 11]
#          |-------|  |-------|  |--------|
#          Row 1       Row 2         Row 3
#
# Array:
#
# [[ 0  1  2  3]
#  [ 4  5  6  7]
#  [ 8  9 10 11]]
#
# F-contiguous:
#
# Memory: [0, 4, 8,  1, 5, 9,  2, 6, 10,  3, 7, 11]
#          |-------|  |-------|  |--------|
#          Col 1       Col 2         Col 3
#
# Array:
#
# [[ 0  1  2  3]
#  [ 4  5  6  7]
#  [ 8  9 10 11]]
#
#
# Example: Examining ndarray attributes
def explore_ndarray_attributes():
    arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.int32)
    print("Array:")
    print(arr)
    print("\nData Buffer Address:", arr.ctypes.data)  # Use ctypes to get the address
    print("Data Type:", arr.dtype)
    print("Shape:", arr.shape)
    print("Strides:", arr.strides)
    print("Flags:", arr.flags)

    # Create a non-contiguous array (view with a slice)
    sliced_arr = arr[::2, 1:3]  # Every other row, columns 1 and 2
    print("\nSliced Array (Non-contiguous):")
    print(sliced_arr)
    print("Sliced Array Shape:", sliced_arr.shape)
    print("Sliced Array Strides:", sliced_arr.strides)  # Strides will be different!
    print("Sliced Array Flags:", sliced_arr.flags)
    # Show how strides work
    print("\nAccessing element (1,1) using strides:")
    element_11 = arr[1, 1]
    print(f"arr[1,1] = {element_11}")

    # Calculate the offset in bytes:
    offset_bytes = arr.strides[0] * 1 + arr.strides[1] * 1
    print(f"Offset in bytes to element (1,1): {offset_bytes}")

    # Use ctypes to access the memory directly.  This is for illustration
    # and deep understanding *only*.  Don't do this in normal code.
    address_of_element_11 = arr.ctypes.data + offset_bytes
    # Get a pointer to the data as a 32-bit integer (dtype of the array)
    int_pointer = ctypes.cast(address_of_element_11, ctypes.POINTER(ctypes.c_int32))
    # Dereference the pointer to get the value
    value_at_address = int_pointer.contents.value
    print(f"Value at calculated memory address: {value_at_address}")
    assert value_at_address == element_11
    print("Contiguous Check (C):", arr.flags['C_CONTIGUOUS'])
    print("Contiguous Check (F):", arr.flags['F_CONTIGUOUS'])

    # Example of creating an F-contiguous array
    f_arr = np.asfortranarray(arr)
    print("\nFortran Contiguous Array:")
    print(f_arr)
    print("F-contiguous strides:", f_arr.strides)
    print("Contiguous Check (C):", f_arr.flags['C_CONTIGUOUS'])
    print("Contiguous Check (F):", f_arr.flags['F_CONTIGUOUS'])
    return arr #Returning arr so that it can be used in other functions

# -----------------------------------------------------------------------------
# NumPy Internals: Data Types
# -----------------------------------------------------------------------------
#
# NumPy provides a rich set of data types, which are essential for
# representing numerical data efficiently.
#
# Key aspects of NumPy data types:
#
# -   Mapping to C types: NumPy data types are often thin wrappers around
#     C data types, allowing for efficient memory storage and computation.
#     For example, `np.int32` maps to C's `int32_t`.
#
# -   Fixed size: Each NumPy data type has a fixed size in bytes
#     (e.g., `np.int8` is 1 byte, `np.float64` is 8 bytes).  This fixed
#     size is crucial for efficient memory management and vectorized
#     operations.
#
# -   Endianness: NumPy supports both little-endian and big-endian
#     representations.  Endianness refers to the order in which bytes
#     are stored in memory.  This is important when working with data
#     from different systems.
#
# -   Type promotion: When performing operations on arrays with different
#     data types, NumPy automatically promotes the data to a common type
#     to avoid data loss.  The rules for type promotion can sometimes
#     lead to unexpected results if not understood.
#
# Example: Exploring NumPy data types
def explore_numpy_dtypes():
    # Integer types
    int8_arr = np.array([1, 2, 3], dtype=np.int8)
    int16_arr = np.array([1, 2, 3], dtype=np.int16)
    int32_arr = np.array([1, 2, 3], dtype=np.int32)
    int64_arr = np.array([1, 2, 3], dtype=np.int64)

    print("Integer Types:")
    print(f"int8:  {int8_arr.dtype}, size: {int8_arr.itemsize} bytes")
    print(f"int16: {int16_arr.dtype}, size: {int16_arr.itemsize} bytes")
    print(f"int32: {int32_arr.dtype}, size: {int32_arr.itemsize} bytes")
    print(f"int64: {int64_arr.dtype}, size: {int64_arr.itemsize} bytes")

    # Floating-point types
    float32_arr = np.array([1.0, 2.0, 3.0], dtype=np.float32)
    float64_arr = np.array([1.0, 2.0, 3.0], dtype=np.float64)  # Also np.double

    print("\nFloating-Point Types:")
    print(f"float32: {float32_arr.dtype}, size: {float32_arr.itemsize} bytes")
    print(f"float64: {float64_arr.dtype}, size: {float64_arr.itemsize} bytes")

    # Other types
    bool_arr = np.array([True, False, True], dtype=np.bool_)
    complex64_arr = np.array([1+1j, 2+2j], dtype=np.complex64)
    complex128_arr = np.array([1+1j, 2+2j], dtype=np.complex128) # Also np.cdouble
    string_arr = np.array(['a', 'b', 'c'], dtype='S3') # Fixed-length string, 3 bytes

    print("\nOther Types:")
    print(f"bool:      {bool_arr.dtype}, size: {bool_arr.itemsize} byte")
    print(f"complex64:   {complex64_arr.dtype}, size: {complex64_arr.itemsize} bytes")
    print(f"complex128:  {complex128_arr.dtype}, size: {complex128_arr.itemsize} bytes")
    print(f"string (S3): {string_arr.dtype}, size: {string_arr.itemsize} bytes")

    # Type promotion example
    mixed_arr = np.array([1, 2.0, 3], dtype=np.float32) #Upcast to float
    print("\nType Promotion Example:")
    print(mixed_arr.dtype)  # Output: float64, int32 promotes to float64
    print(mixed_arr)

    mixed_arr_2 = np.array([1, 2.0, 3], dtype=np.complex128) #Upcast to complex
    print("\nType Promotion to Complex Example:")
    print(mixed_arr_2.dtype)
    print(mixed_arr_2)
# -----------------------------------------------------------------------------
# NumPy Internals: Memory Management
# -----------------------------------------------------------------------------
#
# NumPy's memory management is crucial for handling large arrays efficiently.
#
# Key aspects:
#
# -   Allocation: NumPy allocates memory for arrays using a memory allocator.
#     NumPy uses its own memory manager, which sits on top of the system's
#     memory allocator (e.g., malloc in C). This allows NumPy to optimize
#     memory allocation for arrays.
#
# -   Deallocation: When a NumPy array is no longer needed, its memory is
#     deallocated.  Python's garbage collector automatically handles
#     deallocation when an array's reference count reaches zero.
#
# -   Memory reuse: NumPy tries to reuse memory buffers when possible.
#     For example, some operations (like slicing) create views of existing
#     arrays rather than allocating new memory.  This is a key
#     optimization.
#
# -   `np.empty()`:  This function creates a new array without initializing
#      its elements.  It's faster than `np.zeros()` or `np.ones()` because
#      it doesn't fill the array with zeros or ones.  However, the
#      contents of the array are undefined (whatever was in that memory
#      location previously).
#
# -   `np.zeros()` and `np.ones()`: These functions allocate memory and
#     initialize the array elements to zero or one, respectively.
#
# -   `np.copy()`:  This function explicitly copies the data of an array
#     into a new memory buffer.  This is necessary when you need a
#     completely independent copy of an array.
#
# Example: Memory allocation and views
def explore_numpy_memory_management():
    # Creating arrays
    a = np.zeros((2, 3), dtype=np.float64)  # Allocate and initialize to zero
    b = np.ones((2, 3), dtype=np.int32)   # Allocate and initialize to one
    c = np.empty((2, 3), dtype=np.float64)  # Allocate, but don't initialize
    print("Array a (zeros):")
    print(a)
    print("\nArray b (ones):")
    print(b)
    print("\nArray c (empty - uninitialized):")
    print(c) #will print whatever values were in memory

    # Views and copies
    d = a[0, :]  # d is a view of the first row of a
    e = a.copy()  # e is a copy of a

    print("\nOriginal array a:")
    print(a)
    print("\nView d (first row of a):")
    print(d)
    print("\nCopy e:")
    print(e)

    # Modify the view
    d[0] = 99.0
    print("\nAfter modifying view d:")
    print("Array a:")
    print(a)  # a is changed because d is a view
    print("\nView d:")
    print(d)
    print("\nCopy e:")
    print(e)  # e is unchanged because it's a copy

    # Demonstrate memory address differences
    print("\nMemory addresses:")
    print(f"Address of a: {a.__array_interface__['data'][0]}")
    print(f"Address of d: {d.__array_interface__['data'][0]}")  # Same as a[0,:]
    print(f"Address of e: {e.__array_interface__['data'][0]}")  # Different from a

# -----------------------------------------------------------------------------
# NumPy Internals: Vectorization
# -----------------------------------------------------------------------------
#
# Vectorization is a fundamental concept in NumPy that allows for efficient
# execution of operations on entire arrays rather than individual elements.
#
# How vectorization works:
#
# -   Element-wise operations:  NumPy's universal functions (ufuncs) are
#     designed to operate element-wise on arrays.  Examples include
#     `np.add()`, `np.multiply()`, `np.sin()`, etc.
#
# -   Looping in C:  Instead of Python loops, NumPy's vectorized
#     operations use highly optimized C loops (or even assembly code)
#     to perform the calculations.  This eliminates the overhead of the
#     Python interpreter.
#
# -   SIMD instructions:  NumPy can take advantage of Single Instruction,
#     Multiple Data (SIMD) instructions available on modern CPUs.  SIMD
#     allows the CPU to perform the same operation on multiple data
#     elements simultaneously, significantly speeding up computations.
#
# -   Broadcasting:  Vectorization is often combined with broadcasting
#     to perform operations on arrays with different shapes.
#
# Why vectorization is important:
#
# -   Performance: Vectorized operations are orders of magnitude faster
#     than equivalent operations performed with Python loops.
# -   Conciseness: Vectorized code is often much more concise and easier
#     to read than code with explicit loops.
#
# Example: Vectorization vs. Python loop
def demonstrate_vectorization(size=1000000):
    a = np.random.rand(size)
    b = np.random.rand(size)

    # Using a Python loop (slow)
    def python_loop_sum(x, y):
        result = np.zeros_like(x)
        for i in range(len(x)):
            result[i] = x[i] + y[i]
        return result

    # Using NumPy's vectorized addition (fast)
    def numpy_vectorized_sum(x, y):
        return x + y  # Or np.add(x, y)

    # Time the Python loop
    start_time = time.time()
    result_python = python_loop_sum(a, b)
    python_time = time.time() - start_time
    print(f"Python loop sum: {python_time:.4f} seconds")

    # Time the vectorized operation
    start_time = time.time()
    result_numpy = numpy_vectorized_sum(a, b)
    numpy_time = time.time() - start_time
    print(f"NumPy vectorized sum: {numpy_time:.4f} seconds")

    print(f"Speedup: {python_time / numpy_time:.2f}x")
    # Verify the results are the same (within a tolerance)
    np.testing.assert_allclose(result_python, result_numpy)
import time
# -----------------------------------------------------------------------------
# Broadcasting Rules
# -----------------------------------------------------------------------------
#
# Broadcasting is a powerful feature of NumPy that allows you to perform
# operations on arrays with different shapes.  It avoids the need to
# explicitly reshape arrays, making your code more concise and efficient.
#
# Broadcasting Rules:
#
# When operating on two arrays, NumPy compares their shapes element-wise.
# It starts with the trailing dimensions and works its way forward.
# Two dimensions are compatible when:
#
# 1.  They are equal, or
# 2.  One of them is 1.
#
# If these conditions are not met, a `ValueError: operands could not be
# broadcast together` exception is raised.
#
# If one of the arrays has fewer dimensions than the other, the shape of
# the array with fewer dimensions is padded with ones on its leading
# (left) side.
#
# After the shape comparison, the size of the resulting array is the
# maximum of the sizes along each dimension of the input arrays.
#
# Broadcasting Example (Conceptual):
#
# Example 1:
# A: (2, 3)
# B: (3,)       # B is treated as (1, 3)
# Result: (2, 3)
#
# Example 2:
# A: (4, 3)
# B: (4, 1)     # B is broadcasted along the second dimension
# Result: (4, 3)
#
# Example 3:
# A: (2, 1, 4)
# B: (3, 4)     # B is treated as (1, 3, 4)
# Result: (2, 3, 4)
#
# Visual Representation:
#
# Example 1: A (2,3) + B (3,)
#
# A: [[1 2 3]    B: [10 20 30]  Broadcasted B: [[10 20 30]
#     [4 5 6]]                   [10 20 30]]
#
# Result: [[11 22 33]
#          [14 25 36]]
#
# Example 2: A (4,3) + B (4,1)
#
# A: [[1 2 3]    B: [[10]       Broadcasted B: [[10 10 10]
#     [4 5 6]        [20]                       [20 20 20]
#     [7 8 9]        [30]                       [30 30 30]
#    [10 11 12]]       [40]]                      [40 40 40]]
#
# Result: [[11 12 13]
#          [24 25 26]
#          [37 38 39]
#         [50 51 52]]
#
# How Broadcasting Works Internally:
#
# Broadcasting does *not* create copies of the arrays in memory.  Instead,
# NumPy creates *views* of the arrays with adjusted strides.  The
# `strides` of the arrays are manipulated so that the dimensions with
# size 1 are "virtually repeated" during the operation.  This makes
# broadcasting very memory-efficient.
#
# Example: Broadcasting demonstration
def demonstrate_broadcasting():
    a = np.array([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)
    b = np.array([10, 20, 30])            # Shape (3,)
    c = np.array([[10], [20]])          # Shape (2, 1)

    print("Array a (shape: (2, 3)):")
    print(a)
    print("\nArray b (shape: (3,)):")
    print(b)
    print("\nArray c (shape: (2, 1)):")
    print(c)

    # Broadcasting b to (2, 3)
    result1 = a + b
    print("\na + b (broadcasting b to (2, 3)):")
    print(result1)

    # Broadcasting c to (2, 3)
    result2 = a + c
    print("\na + c (broadcasting c to (2, 3)):")
    print(result2)

    # Broadcasting with higher dimensions
    d = np.array([1, 2, 3, 4])          # Shape (4,)
    e = np.array([[10], [20], [30]])      # Shape (3, 1)
    f = np.arange(24).reshape(2, 3, 4)  # Shape (2, 3, 4)

    print("\nArray d (shape: (4,)):")
    print(d)
    print("\nArray e (shape: (3, 1)):")
    print(e)
    print("\nArray f (shape: (2, 3, 4)):")
    print(f)

    result3 = f + d  # d is broadcasted to (2, 3, 4)
    print("\nf + d (broadcasting d to (2, 3, 4)):")
    print(result3)

    result4 = f + e  # e is broadcasted to (2, 3, 4)
    print("\nf + e (broadcasting e to (2, 3, 4)):")
    print(result4)
    return a,b,c,d,e,f #returning the arrays so that they can be used in other functions

# -----------------------------------------------------------------------------
# When to Use/Not Use NumPy
# -----------------------------------------------------------------------------
#
# When to Use NumPy:
#
# -   Numerical computation: NumPy is the foundation for efficient
#     numerical computation in Python.  Use it for any task involving
#     arrays of numbers, including:
#     -   Mathematical operations (e.g., arithmetic, trigonometry,
#         exponentiation)
#     -   Linear algebra (e.g., matrix multiplication, decomposition,
#         eigenvalue problems)
#     -   Statistics (e.g., mean, standard deviation, correlation)
#     -   Fourier analysis
#     -   Random number generation
#
# -   Large datasets: NumPy is designed to handle large datasets efficiently.
#     Its vectorized operations and memory management make it much faster
#     than using Python loops for processing large amounts of numerical
#     data.
#
# -   Interoperability: NumPy arrays can be easily interfaced with other
#     libraries and languages, such as:
#     -   SciPy (scientific computing library)
#     -   Matplotlib (plotting library)
#     -   Pandas (data analysis library)
#     -   Cython (for writing C extensions)
#     -   Fortran and C libraries
#
# When to Be Cautious About NumPy:
#
# -   Small arrays: For very small arrays (e.g., a few elements), the
#     overhead of NumPy might outweigh the benefits of vectorization.
#     In such cases, using standard Python lists and loops might be
#     sufficient.  However, it's generally good practice to use NumPy
#     even for small arrays, as your code will be more consistent and
#     easier to scale if your data grows.
#
# -   Non-numerical data: NumPy is designed for numerical data.  If you
#     are working with arrays of strings, objects, or other non-numerical
#     data types, NumPy's performance benefits will be limited.  In such
#     cases, consider using Python lists or other data structures.  However,
#if you can represent your data numerically (e.g., using one-hot
#     encoding for categorical data), you can still use NumPy effectively.
#
# -   Excessive memory usage:  While NumPy is memory-efficient, some
#     operations can lead to increased memory consumption, especially when
#     broadcasting large arrays.  Be mindful of the size of your arrays
#     and try to avoid creating unnecessary copies of data.  Use views
#     and in-place operations whenever possible.
#
# When to Consider Alternatives:
#
# -   GPU acceleration: For very large datasets and computationally
#     intensive tasks, you might benefit from using libraries that can
#     leverage the power of GPUs:
#     -   PyTorch:  Provides GPU-accelerated tensor computation with
#         automatic differentiation, making it suitable for deep learning.
#     -   TensorFlow: Another powerful library for machine learning,
#         also with strong GPU support.
#     -   CuPy: A NumPy-compatible array library that runs on NVIDIA GPUs.
#
# -   Distributed computing: If your data is too large to fit into the
#     memory of a single machine, you might need to use distributed
#     computing frameworks:
#     -   Dask: A flexible library for parallel computing in Python.
#     -   Spark: A powerful engine for big data processing.
#
# -   Sparse data: If your data contains many zeros, you can save
#     memory by using sparse matrix formats:
#     -   SciPy's `sparse` module: Provides various sparse matrix
#         formats and operations.
#
# -----------------------------------------------------------------------------
# Challenge: Rewrite a CPU-bound image processing loop using NumPy + broadcasting
# -----------------------------------------------------------------------------
#
# Challenge Description:
#
# You are given a grayscale image represented as a 2D NumPy array.  You need to
# apply a contrast adjustment to the image.  The contrast adjustment is defined
# by the following formula:
#
#   output_pixel = (input_pixel - mean) * contrast_factor + mean
#
# where:
#
# -   `input_pixel` is the grayscale value of a pixel (0-255).
# -   `mean` is the mean grayscale value of the entire image.
# -   `contrast_factor` is a floating-point number that controls the contrast.
#
# Your task is to write a function that performs this contrast adjustment
# efficiently using NumPy and broadcasting.  Avoid using explicit Python loops.
#
# Input:
#
# -   `image`: A 2D NumPy array of shape (height, width) representing the grayscale image.
#     The data type of the array is `np.uint8`.
# -   `contrast_factor`: A float representing the contrast adjustment factor.
#
# Output:
#
# -   A new 2D NumPy array of the same shape as the input image, representing the
#     contrast-adjusted image.  The data type of the array should be `np.uint8`.
#
# Example:
#
# Input Image:
# [[100 150 200]
#  [ 50 100 150]
#  [ 20 50 100]]
#
# contrast_factor = 1.5
#
# Mean = 103.33
#
# Output Image:
# [[145 215 285]
#  [ 65 145 215]
#  [ 15  65 145]]
#
# Since the values need to be between 0-255, you should clip them.
#
#
# Solution:
#
def adjust_contrast_numpy(image, contrast_factor):
    """
    Adjusts the contrast of a grayscale image using NumPy and broadcasting.

    Args:
        image: A 2D NumPy array representing the grayscale image (np.uint8).
        contrast_factor: The contrast adjustment factor (float).

    Returns:
        A new 2D NumPy array representing the contrast-adjusted image (np.uint8).
    """
    mean = np.mean(image)
    adjusted_image = (image - mean) * contrast_factor + mean
    # Clip the values to the range 0-255 and convert to uint8
    return np.clip(adjusted_image, 0, 255).astype(np.uint8)

def create_sample_image():
    """
    Creates a sample grayscale image for testing.

    Returns:
        A 2D NumPy array representing a grayscale image.
    """
    image = np.array([[100, 150, 200],
                      [50, 100, 150],
                      [20, 50, 100]], dtype=np.uint8)
    return image

if __name__ == "__main__":
    print("Exploring NumPy Internals:")
    arr = explore_ndarray_attributes()
    explore_numpy_dtypes()
    explore_numpy_memory_management()
    demonstrate_vectorization()
    a,b,c,d,e,f = demonstrate_broadcasting() #storing the arrays
    print("\nRunning Challenge:")
    sample_image = create_sample_image()
    contrast_factor = 1.5
    adjusted_image = adjust_contrast_numpy(sample_image, contrast_factor)

    print("Original Image:")
    print(sample_image)
    print("\nAdjusted Image (contrast factor = 1.5):")
    print(adjusted_image)

    # Further tests to ensure the solution is robust
    def test_adjust_contrast():
        # Test with a simple image and a contrast factor of 1 (should be no change)
        image1 = np.array([[10, 20], [30, 40]], dtype=np.uint8)
        adjusted1 = adjust_contrast_numpy(image1, 1.0)
        np.testing.assert_array_equal(adjusted1, image1)

        # Test with a contrast factor of 0 (should result in a uniform image)
        image2 = np.array([[50, 100], [150, 200]], dtype=np.uint8)
        adjusted2 = adjust_contrast_numpy(image2, 0.0)
        expected_value = np.mean(image2)
        expected_image = np.full_like(image2, expected_value, dtype=np.uint8)
        np.testing.assert_array_equal(adjusted2, expected_image)

        # Test with a contrast factor greater than 1
        image3 = np.array([[0, 128, 255]], dtype=np.uint8)
        adjusted3 = adjust_contrast_numpy(image3, 2.0)
        expected3 = np.array([[  0, 128, 255]], dtype=np.uint8) #after clipping
        np.testing.assert_array_equal(adjusted3, expected3)
        print("All tests passed!")
    test_adjust_contrast()