# -*- coding: utf-8 -*-
"""Challenge: Parallel File Compressor (multiprocessing and asyncio)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JnFNrjKQbtPyp8Fz1ptILr8dxqRARZw7
"""

"""
Day 3, Session 3, Topic 5: Challenge - Build a Parallel File Compressor

This file presents a challenge to build a parallel file compressor using
multiprocessing and asyncio.  It provides a detailed explanation of the
problem, outlines the requirements, and offers a solution that leverages
both libraries for optimal performance.
"""

import asyncio
import concurrent.futures
import os
import subprocess  # For calling gzip
import tempfile
from typing import List, Tuple

# -----------------------------------------------------------------------------
# Challenge Description
# -----------------------------------------------------------------------------
#
# The challenge is to create a Python program that can compress multiple files
# in parallel, taking advantage of both multi-core CPUs (using
# `multiprocessing`) and asynchronous I/O operations (`asyncio`).
#
# The program should:
#
# 1.  Accept a list of file paths as input.
# 2.  Compress each file using the `gzip` compression utility.
# 3.  Store the compressed files in a temporary directory.
# 4.  Return a list of paths to the compressed files.
#
# The goal is to achieve the fastest possible compression time by utilizing
# parallelism for the CPU-bound compression tasks and asynchronous operations
# for managing the file I/O.
#
# -----------------------------------------------------------------------------
# Requirements
# -----------------------------------------------------------------------------
#
# -   The program must use `multiprocessing` to compress files in parallel.
#     Each file compression should be handled by a separate process to
#     fully utilize multi-core CPUs.
#
# -   The program must use `asyncio` to manage the file I/O operations
#     (creating temporary files, writing compressed data).  While the
#     compression itself is CPU-bound, the file operations can be
#     performed asynchronously to avoid blocking the main thread.
#
# -   The program should handle potential errors during file compression
#     and I/O operations gracefully.
#
# -   The program should clean up the temporary files after the compression
#     is complete.
#
# -   The program should provide a clear interface for the user to specify
#     the input files and retrieve the paths to the compressed files.
#
# -----------------------------------------------------------------------------
# Solution
# -----------------------------------------------------------------------------
#
# The following solution combines `multiprocessing` and `asyncio` to address
# the challenge:
#
# 1.  The `compress_file` function performs the CPU-bound gzip compression
#     using the `subprocess` module.  This function is designed to be
#     executed in a separate process.
#
# 2.  The `async_copy_file` function handles the asynchronous creation of
#     temporary files and writing of compressed data using asyncio's
#     `loop.run_in_executor`.
#
# 3.  The `parallel_compress_files` coroutine orchestrates the parallel
#     compression process using `multiprocessing` and `asyncio`.  It:
#     - Creates a temporary directory for the compressed files.
#     - Creates a process pool using `concurrent.futures.ProcessPoolExecutor`.
#     - Submits the `compress_file` tasks to the process pool.
#     - Uses `asyncio.gather` to wait for the compression tasks to complete.
#     - Uses `asyncio.to_thread` to run the file copying in a separate thread.
#     - Returns the paths to the compressed files.
#
# 4.  The main part of the script calls `asyncio.run` to execute the
#     `parallel_compress_files` coroutine.
#
def compress_file(input_file: str, temp_dir: str) -> str:
    """
    Compresses a file using gzip and saves it to a temporary directory.
    This function is intended to be run in a separate process.

    Args:
        input_file: Path to the file to compress.
        temp_dir: Path to the temporary directory.

    Returns:
        Path to the compressed file, or None on error.
    """
    try:
        # Generate a unique filename for the compressed file in the temp dir
        output_file = os.path.join(temp_dir, os.path.basename(input_file) + ".gz")
        # Use subprocess.run to execute the gzip command.
        result = subprocess.run(
            ["gzip", "-c", input_file, ">", output_file],  # Use shell redirection
            shell=True,  # Enable shell redirection
            check=True,  # Raise an exception on non-zero exit code
            capture_output=True,  # Capture standard output and error
            # text=True,  # Remove text mode, handle binary data
        )
        print(f"Process {os.getpid()}: Compressed {input_file} to {output_file}")
        return output_file
    except subprocess.CalledProcessError as e:
        print(f"Process {os.getpid()}: Error compressing {input_file}: {e}")
        print(f"Process {os.getpid()} Stderr: {e.stderr.decode()}")  # Decode binary stderr
        return None
    except Exception as e:
        print(f"Process {os.getpid()}: Unexpected error compressing {input_file}: {e}")
        return None



async def async_copy_file(loop: asyncio.AbstractEventLoop, src_path: str, dest_path: str) -> None:
    """
    Asynchronously copies a file using a thread.

    Args:
        loop: The asyncio event loop.
        src_path: Path to the source file.
        dest_path: Path to the destination file.
    """
    def copy_sync():
        """Synchronous file copy operation."""
        try:
            # Use shutil.copy for the actual file copy
            import shutil
            shutil.copy(src_path, dest_path)
            print(f"Copied {src_path} to {dest_path} in thread")
        except Exception as e:
            print(f"Error copying {src_path} to {dest_path}: {e}")

    # Use loop.run_in_executor to run the synchronous copy in a thread
    await loop.run_in_executor(None, copy_sync)



async def parallel_compress_files(input_files: List[str]) -> List[str]:
    """
    Compresses multiple files in parallel using multiprocessing and asyncio.

    Args:
        input_files: A list of paths to the files to compress.

    Returns:
        A list of paths to the compressed files.
    """
    compressed_files = []
    # Create a temporary directory.
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"Using temporary directory: {temp_dir}")
        loop = asyncio.get_event_loop()

        # Create a process pool.
        with concurrent.futures.ProcessPoolExecutor() as executor:
            # Submit the compression tasks to the process pool.
            futures = [
                loop.run_in_executor(executor, compress_file, input_file, temp_dir)
                for input_file in input_files
            ]
            # Wait for the compression tasks to complete.
            completed_files = await asyncio.gather(*futures)

        compressed_files = [file for file in completed_files if file is not None]

        # Use asyncio.to_thread to copy the files
        copy_tasks = []
        for src, dest in zip(compressed_files, [os.path.join(temp_dir,os.path.basename(f)) for f in compressed_files]):
            copy_tasks.append(asyncio.create_task(async_copy_file(loop, src, dest)))
        await asyncio.gather(*copy_tasks)
        print("All files compressed.")
        return compressed_files



def create_dummy_files(num_files: int, temp_dir: str) -> List[str]:
    """
    Creates dummy files for testing.

    Args:
        num_files: The number of dummy files to create.
        temp_dir: The directory where the files should be created.

    Returns:
        A list of paths to the created dummy files.
    """
    file_paths = []
    for i in range(num_files):
        file_path = os.path.join(temp_dir, f"dummy_file_{i}.txt")
        with open(file_path, "w") as f:
            f.write(f"This is dummy file {i} with some text.\n")
        file_paths.append(file_path)
    return file_paths



async def main():
    """
    Main function to run the parallel file compressor.
    """
    # Create a temporary directory for the dummy files
    with tempfile.TemporaryDirectory() as temp_dir:
        # Create dummy files
        input_files = create_dummy_files(3, temp_dir)

        print(f"Input files: {input_files}")

        # Compress the files in parallel
        compressed_files = await parallel_compress_files(input_files)
        print(f"Compressed files: {compressed_files}")



if __name__ == "__main__":
    import nest_asyncio
    nest_asyncio.apply()  # Patch asyncio to allow nested event loops (for Colab)
    asyncio.run(main())